# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
from sklearn import datasets
iris = datasets.load_iris()

# Create DataFrame
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
data['target'] = iris.target 

data['target'] = data['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})
data=data.sample(frac=1, random_state=42).reset_index(drop=True)
x=data[[x for x in data.columns if x != "target"]]
y=data["target"]
from sklearn.decomposition import PCA
 
pca = PCA(n_components = 2)
red_x=pca.fit_transform(x)
import plotly.express as px
# Create DataFrame from PCA results
pca_df = pd.DataFrame(data=red_x, columns=['PC1', 'PC2'])

# Plot using Plotly
fig = px.scatter(pca_df, x='PC1', y='PC2', title='PCA Visualization',color=y)
fig.show()
x_train=red_x[0:120]
x_train
array([[ 0.92172892, -0.18273779],
       [-2.19982032,  0.87283904],
       [ 3.79564542,  0.25732297],
       [ 0.81329065, -0.1633503 ],
       [ 1.33202444,  0.24444088],
       [-2.4099325 ,  0.41092426],
       [-0.17392537, -0.25485421],
       [ 1.9222678 ,  0.40920347],
       [ 0.94473373, -0.54314555],
       [ 0.13642871, -0.31403244],
       [ 1.66177415,  0.24222841],
       [-2.78610927, -0.235112  ],
       [-2.62523805,  0.59937002],
       [-2.6727558 , -0.11377425],
       [-2.5879864 ,  0.51356031],
       [ 1.09506066,  0.28346827],
       [ 2.35000592, -0.04026095],
       [ 0.04522698, -0.58383438],
       [ 0.64166908, -0.41824687],
       [ 2.15943764, -0.21727758],
       [-2.63198939, -0.19696122],
       [ 1.29113206, -0.11666865],
       [-2.46882007,  0.13095149],
       [ 2.12360872, -0.20972948],
       [ 3.23067366,  1.37416509],
       [ 1.94410979,  0.1875323 ],
       [ 2.32122882, -0.2438315 ],
       [ 2.56301338,  0.2778626 ],
       [-2.71445143, -0.2502082 ],
       [-2.58739848, -0.20431849],
       [-3.21593942,  0.13346807],
       [-2.38603903,  1.33806233],
       [ 0.92786078,  0.46717949],
       [-2.61275523,  0.01472994],
       [-2.99740655, -0.34192606],
       [ 1.52716661, -0.37531698],
       [ 0.93248853,  0.31833364],
       [-2.56231991,  0.36771886],
       [-2.72871654,  0.32675451],
       [-2.64886233,  0.81336382],
       [ 1.41523588, -0.57491635],
       [ 0.80685831,  0.19418231],
       [ 1.22069088,  0.40761959],
       [-2.62352788,  0.81067951],
       [-2.50694709,  0.6450689 ],
       [-0.18962247, -0.68028676],
       [ 1.44416124, -0.14341341],
       [ 1.90509815,  0.04930053],
       [ 0.90017437,  0.32850447],
       [ 2.91675097,  0.78279195],
       [ 0.37621565, -0.29321893],
       [ 3.39703874,  0.55083667],
       [ 0.66028376, -0.35296967],
       [-2.68412563,  0.31939725],
       [ 3.49992004,  0.4606741 ],
       [ 0.23610499, -0.33361077],
       [-2.63953472,  0.31203998],
       [-2.77010243,  0.26352753],
       [-2.20948924,  0.43666314],
       [-0.50784088, -1.26597119],
       [ 1.38876613, -0.20439933],
       [-2.35575405, -0.03728186],
       [-2.50666891, -0.14601688],
       [-2.30273318,  0.09870885],
       [ 0.35698149, -0.50491009],
       [-2.59000631,  0.22904384],
       [ 0.33193448, -0.21265468],
       [ 3.48705536,  1.17573933],
       [-2.83946217, -0.22794557],
       [ 0.64257601,  0.01773819],
       [ 1.34616358, -0.77681835],
       [-2.59873675,  1.09314576],
       [ 1.16932634, -0.16499026],
       [ 1.41523588, -0.57491635],
       [ 0.26497651, -0.55003646],
       [ 0.58800644, -0.48428742],
       [ 1.90094161,  0.11662796],
       [ 0.18331772, -0.82795901],
       [-2.28085963,  0.74133045],
       [-0.70453176, -1.01224823],
       [ 1.80340195, -0.21563762],
       [-2.70335978,  0.10770608],
       [-2.86624165,  0.06936447],
       [-0.06812649, -0.70517213],
       [ 1.55780216,  0.26749545],
       [-2.63692688, -0.12132235],
       [ 1.58592822, -0.53964071],
       [-2.62614497,  0.16338496],
       [-2.40561449,  0.18887143],
       [ 1.11628318, -0.08461685],
       [-0.90646986, -0.75609337],
       [ 2.42781791,  0.37819601],
       [ 1.38002644, -0.42095429],
       [ 1.78129481, -0.49990168],
       [ 3.07649993,  0.68808568],
       [ 0.16641322, -0.68192672],
       [-2.88638273, -0.57831175],
       [-3.22380374, -0.51139459],
       [ 1.30079171, -0.76114964],
       [ 2.61409047,  0.56090136],
       [-2.74534286, -0.31829898],
       [-2.64829671,  0.31184914],
       [-2.98050204, -0.48795834],
       [ 1.29818388, -0.32778731],
       [ 2.14424331,  0.1400642 ],
       [-2.82053775, -0.08946138],
       [ 2.1655918 ,  0.21627559],
       [ 2.53119273, -0.00984911],
       [-2.88899057, -0.14494943],
       [ 0.98493451, -0.12481785],
       [ 1.08810326,  0.07459068],
       [ 1.25850816, -0.17970479],
       [ 1.28482569,  0.68516047],
       [ 1.90445637,  0.11925069],
       [-2.53814826,  0.50377114],
       [ 2.10761114,  0.37228787],
       [ 0.51169856, -0.10398124],
       [ 1.76434572,  0.07885885],
       [-0.30558378, -0.36826219],
       [-0.0087454 , -0.72308191]])
add Codeadd Markdown
x_test=red_x[120:]
x_test
array([[ 0.8908152 , -0.03446444],
       [-2.84936871, -0.94096057],
       [ 1.04413183,  0.2283619 ],
       [ 0.46480029, -0.67071154],
       [-2.54308575,  0.57941002],
       [ 0.24595768, -0.2685244 ],
       [ 2.93258707,  0.3555    ],
       [ 2.2754305 ,  0.33499061],
       [-2.54370523,  0.43299606],
       [-0.74912267, -1.00489096],
       [ 2.41874618,  0.3047982 ],
       [ 2.38800302,  0.4646398 ],
       [-2.80068412,  0.26864374],
       [ 2.31415471,  0.18365128],
       [-2.71414169, -0.17700123],
       [ 1.46430232,  0.50426282],
       [ 2.84167278,  0.37526917],
       [ 1.97153105, -0.1797279 ],
       [ 0.29900084, -0.34889781],
       [ 1.94968906,  0.04194326],
       [ 0.81509524, -0.37203706],
       [ 0.71485333,  0.14905594],
       [ 1.19900111, -0.60609153],
       [ 1.39018886, -0.28266094],
       [-2.31025622,  0.39134594],
       [ 0.35788842, -0.06892503],
       [ 0.52123224, -1.19275873],
       [-2.64475039,  1.17876464],
       [ 0.23054802, -0.40438585],
       [ 2.61667602,  0.34390315]])
add Codeadd Markdown
y_train=y.iloc[0:120]
y_train
0      versicolor
1          setosa
2       virginica
3      versicolor
4      versicolor
          ...    
115     virginica
116    versicolor
117     virginica
118    versicolor
119    versicolor
Name: target, Length: 120, dtype: object
add Codeadd Markdown
y_test=y.iloc[120:]
y_test
120    versicolor
121        setosa
122    versicolor
123    versicolor
124        setosa
125    versicolor
126     virginica
127     virginica
128        setosa
129    versicolor
130     virginica
131     virginica
132        setosa
133     virginica
134        setosa
135    versicolor
136     virginica
137     virginica
138    versicolor
139     virginica
140    versicolor
141    versicolor
142     virginica
143     virginica
144        setosa
145    versicolor
146     virginica
147        setosa
148    versicolor
149     virginica
Name: target, dtype: object
add Codeadd Markdown
K-Nearest Neighbors
add Codeadd Markdown
def ec_dist(x1,x2):
    return np.sqrt(np.sum((x1 - x2)**2))
#print(ec_dist(np.array([10,10]),np.array([20,20])))   test
add Codeadd Markdown
def knn_s(x_train, x_test,y_train,k):
    predictions=[]
    for sample in x_test:
        distances=[]
        for i in range(len(x_train)):
            distance=ec_dist(sample,x_train[i])
            distances.append((distance,y_train[i]))
        fin_distances=sorted(distances)[:k]
        lables=[x[1] for x in fin_distances]
        predictions.append(max(set(lables), key=lables.count))
    return predictions
​
        
add Codeadd Markdown
from sklearn.metrics import accuracy_score
for k in [3,5,7]:
    y_pred= knn_s(x_train,x_test,y_train,k)
    sct_acc=accuracy_score(y_pred,y_test)
    print(f'The accuracy of our implementation using k= {k} is {sct_acc}')
​
The accuracy of our implementation using k= 3 is 0.9333333333333333
The accuracy of our implementation using k= 5 is 0.9666666666666667
The accuracy of our implementation using k= 7 is 0.9666666666666667
add Codeadd Markdown
from sklearn.neighbors import KNeighborsClassifier
for k in [3,5,7]:
    knn = KNeighborsClassifier(n_neighbors=k) 
    knn.fit(x_train, y_train)
    y_pred = knn.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f'The accuracy of sklearn implementation using k = {k} is {accuracy}')
def ec_dist(x1,x2):
    return np.sqrt(np.sum((x1 - x2)**2))
#print(ec_dist(np.array([10,10]),np.array([20,20])))   test
def knn_s(x_train, x_test,y_train,k):
    predictions=[]
    for sample in x_test:
        distances=[]
        for i in range(len(x_train)):
            distance=ec_dist(sample,x_train[i])
            distances.append((distance,y_train[i]))
        fin_distances=sorted(distances)[:k]
        lables=[x[1] for x in fin_distances]
        predictions.append(max(set(lables), key=lables.count))
    return predictions
from sklearn.metrics import accuracy_score
for k in [3,5,7]:
    y_pred= knn_s(x_train,x_test,y_train,k)
    sct_acc=accuracy_score(y_pred,y_test)
    print(f'The accuracy of our implementation using k= {k} is {sct_acc}')
from sklearn.neighbors import KNeighborsClassifier
for k in [3,5,7]:
    knn = KNeighborsClassifier(n_neighbors=k) 
    knn.fit(x_train, y_train)
    y_pred = knn.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f'The accuracy of sklearn implementation using k = {k} is {accuracy}')
pd.DataFrame(red_x).describe()
# Calculate mean and standard deviation for each column
mean_values = red_x.mean()
std_dev_values = red_x.std()

# Normalize each column
normalized_x = (red_x - mean_values) / std_dev_values
pd.DataFrame(normalized_x).describe()
x_train_norm=normalized_x[0:120]
x_test_norm=normalized_x[120:]
​
from sklearn.metrics import accuracy_score
for k in [3,5,7]:
    y_pred_norm= knn_s(x_train_norm,x_test_norm,y_train,k)
    sct_acc=accuracy_score(y_pred_norm,y_test)
    print(f'The accuracy of our implementation using k -Normalized-= {k} is {sct_acc}')
for k in [3,5,7]:
    knn = KNeighborsClassifier(n_neighbors=k) 
    knn.fit(x_train_norm, y_train)
    y_pred_norm = knn.predict(x_test_norm)
    accuracy = accuracy_score(y_test, y_pred_norm)
    print(f'The accuracy of sklearn implementation using k -Normalized- = {k} is {accuracy}')
